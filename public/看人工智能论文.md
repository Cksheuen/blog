# AHDHS-Stacking

这是一种基于**和谐搜索（HS）算法**和**堆叠**的糖尿病分类和诊断[的集成学习](https://www.sciencedirect.com/topics/computer-science/ensemble-learning)框架 ，包括**特征选择**和**基本学习器组合优化**两个阶段。为了提高模型的整体性能，将所有基础学习者的平均性能作为特征选择目标，并使用自适应超参数策略来加速迭代过程。然后使用 HS 进行优化，以找到基本学习器的最佳组合，从而提高模型性能，同时降低复杂性。

---

该框架分为两部分：特征选择和基础学习者组合优化。为了创建一个相对平衡和强大的特征子集，该框架优化了所有基础学习器的平均性能，并在特征选择阶段应用了改进的 HS 算法。这可确保每个基本学习者在特征子集上表现得更好。在整个分类器学习阶段，KNN被用作元学习器，HS也用于连续迭代组合基础学习器的最佳子集，在子集中融合模型预测概率后产生最终的预测结果。

---

## 数据清洗

首先，对原始数据集进行缺失值处理。包括**数据预处理**和**相关性分析**。

---

### 数据预处理

在数据预处理阶段，对数据集进行了探索性数据分析（EDA）。该分析包括涉及每个单独特征的缺失比例、均值、方差、中位数、四分位数和其他相关统计度量的计算。执行此程序的首要目标是全面了解原始数据集中要素的空间分布。

---

PIDD 数据集中的几个样本表现出缺失的属性，包括葡萄糖、血压和皮肤厚度等。值得注意的是，胰岛素特征表现出特别明显的缺失，缺失率为 48.7%。在解决缺失值问题时，本研究与流行方法一致，采用统计得出的相应属性的中值进行插补。这种方法确保了[数据分布](https://www.sciencedirect.com/topics/computer-science/data-distribution)的一致性，与大多数相关研究中观察到的做法保持一致。随后，根据四分位数计算四[分位](https://www.sciencedirect.com/topics/computer-science/interquartile-range)距 （IQR） 值，用于识别每个单独特征中的潜在异常值。

---

$$
IQR = Q_3 - Q_1
$$
其中 Q1 表示数据集的下四分位数，对应于第 25 个百分位数，而 Q3 对应于上四分位数，表示数据的第 75 个百分位数。

---

PIDD中包含的8个连续变量中存在大量异常值，这些异常值也成为噪声信息，降低了模型的性能，并且异常值的处理方式与缺失值相同，并且使用相同的属性中值替换， 减少由于统计误差导致模型收敛困难的情况。由于各个特征表现出不同的尺度，直接拟合过程带来了特定的挑战。因此，在这项研究中，采用Z分数归一化技术对所有特征进行标准化，得出每个特征的平均值为0，方差为1。
$$
Z = \frac{z - \mu}{\delta}
$$
哪里𝜇表示平均值（平均值）和𝜎表示标准差。

---

关于CWMDD，本研究编码了两个离散变量，即性别和UGLU。编码涉及将值 0 分配给男性，将 1 分配给女性。尿糖水平分为六个不同的等级，反映了不同程度的严重程度：阴性（-）、弱阳性（±）和正（+,++,+++,++++).在这种分类中，存在额外的”+“符号表示尿液中的葡萄糖浓度逐渐升高。此外，编码是按顺序执行的，包括从 0 到 5 的数字范围。

---

![](https://ars.els-cdn.com/content/image/1-s2.0-S1319157823004275-gr3.jpg)

---

### 相关性分析

[图 3](https://www.sciencedirect.com/science/article/pii/S1319157823004275#fig3) 描绘了数据预处理后 PID 数据集中特征相关性的热图。特征之间统计依赖关系的建立源于[皮尔逊相关系](https://www.sciencedirect.com/topics/computer-science/pearson-correlation-coefficient)数。从图中可以看出，较深的阴影表明两个特征之间的相关性增强。值得注意的是，考虑“年龄”和“怀孕”之间的相关性。然而，这种升高的相关性表明冗余信息过多。因此，在特征选择阶段需要消除上述特征对。此外，我们对类别信息进行了关联区分，我们的分析表明，“葡萄糖”和“胰岛素”在识别糖尿病患者方面表现出明显的鉴别能力。它们与标签信息的相关性分别达到 50% 和 60%。这一发现强调了在特征选择过程中对此类信息进行优先级排序，以提高分类准确性。

---

[图4](https://www.sciencedirect.com/science/article/pii/S1319157823004275#fig4)显示了CWMD数据集中各种特征之间的相关系数。很明显，LDL和TC之间存在实质性的相关性。同样，UGLU和FBG也表现出一定程度的相关性。这一观察结果与我们通常持有的一般假设一致：糖尿病患者可能同时经历FBG和UGLU的异常水平。此外，年龄、UGLU和FBG 与[分类变量](https://www.sciencedirect.com/topics/computer-science/categorical-variable)呈正相关。这与从 𝑝-[表 2](https://www.sciencedirect.com/science/article/pii/S1319157823004275#tbl2) 中所示的值计算。总之，这些发现强调了SC作为糖尿病诊断的关键因素的不足（𝑝>0.05).相反，其余变量的分布在两个队列之间显着差异（𝑝<0.05).

---



![img](https://ars.els-cdn.com/content/image/1-s2.0-S1319157823004275-gr4.jpg)

---

## 特征选择

然后，处理后的数据进入多目标特征选择阶段，利用HS去除冗余特征，实现降维。

框架中的多阶段优化过程是线性的，特征选择阶段和堆叠组合阶段是连续运行的。

## 堆叠组合优化

最后，利用HS优化获得更好的基于堆叠的学习器组合，以提高[分类精度](https://www.sciencedirect.com/topics/computer-science/classification-accuracy)。

---

### HS功能选择

#### AHHS

AHHS（Adaptive Honey Bee Swarm）是一种基于蜂群算法的改进型优化算法。它借鉴了蜜蜂群体在寻找食物和优质巢穴的行为，通过模拟蜜蜂的搜索行为来解决各种优化问题。

---

下面是AHHS算法的基本步骤：

1. **初始化**：初始化蜜蜂群体，设定算法参数，如蜜蜂数量、搜索范围、最大迭代次数等。
2. **种群分布**：将蜜蜂群体随机分布在解空间中的不同位置，这些位置称为“蜜蜂座标”。
3. **评估**：对每个蜜蜂座标进行评估，计算其对应解的适应度值，即问题的目标函数值。
4. **挑选**：根据蜜蜂座标的适应度值，选择优质的座标作为“精英蜜蜂”，并将其所在位置保存作为全局最优解。

---

5. **搜索**：蜜蜂根据不同的搜索策略进行搜索。通常，蜜蜂会根据当前位置附近的信息，选择下一个候选位置进行探索。全局搜索和局部搜索策略会同时进行，以保证对解空间的全面探索。
6. **更新**：根据搜索结果，更新蜜蜂座标的位置，并重新评估新位置的适应度值。
7. **终止条件**：检查是否满足终止条件，如达到最大迭代次数或者收敛到满意解的精度。
8. **输出**：输出最优解或者近似最优解，算法结束。

---

AHHS相对于传统蜂群算法的改进主要体现在以下几个方面：

- **自适应参数调整**：AHHS通过动态调整算法参数，使其能够自适应地适应不同的问题和不同的搜索空间，提高了算法的鲁棒性和适用性。
- **混合搜索策略**：AHHS结合了全局搜索和局部搜索策略，充分利用了不同搜索策略的优势，使得算法能够更快地收敛到全局最优解。
- **精英策略**：引入精英策略，确保在搜索过程中保留最优解，避免算法陷入局部最优解。
- **并行计算**：AHHS可以通过并行计算的方式加速搜索过程，提高了算法的效率和性能。

综上所述，AHHS作为一种改进的蜂群算法，在解决各种优化问题时具有较高的搜索效率和收敛速度，广泛应用于工程、科学等领域的优化问题求解中。

---

#### HMS

谐波存储器空间（Harmonic Memory Space，HMS）是一种在神经网络领域中应用的概念，用于存储和处理与模式识别、关联记忆等相关的信息。HMS 的基本思想是将信息存储为一组谐波向量，这些向量之间存在特定的谐波关系，从而能够实现对模式的高效存储和检索。

---

生成 HMS 随机谐波向量的过程通常分为以下几个步骤：

1. **随机初始化**：
   - 首先，需要随机初始化一组向量作为初始的谐波向量。这些向量可以是具有一定范围内的随机值的实数向量，或者是由随机二进制值组成的二值向量。
2. **谐波关系建立**：
   - 接下来，需要建立这些向量之间的谐波关系。谐波关系指的是这些向量之间存在特定的关系，使得它们可以相互调制和交互影响。
   - 一种常见的方法是通过线性或非线性的方式将初始向量进行组合，从而生成新的谐波向量。这些组合操作可以是矩阵乘法、逐元素相乘、非线性激活函数等。

---

3. **谐波向量更新**：

- 在谐波关系建立之后，可以通过不断迭代更新谐波向量的数值，以使得它们逐渐趋向于稳定的状态。这个过程可以通过不断重复应用谐波关系来实现。

4. **存储和检索**：

- 一旦谐波向量达到稳定状态，它们可以用于存储和检索模式信息。存储模式时，将模式向量与谐波向量进行调制，得到存储后的谐波向量；检索模式时，利用存储的谐波向量与输入向量进行比较，以判断它们之间的相似度。

---

总的来说，生成 HMS 随机谐波向量的过程涉及到随机初始化、谐波关系的建立和更新、以及存储和检索模式信息等步骤。这种方法可以有效地存储和处理与模式识别相关的信息，具有一定的理论和应用价值。

---

#### 论文中实现

通过在算法迭代中动态调整超参数[HMCR](https://www.sciencedirect.com/topics/computer-science/harmony-memory)和PAR，即使与当前最优解相同，也允许随机生成的[解向量](https://www.sciencedirect.com/topics/computer-science/solution-vector)有变化的机会，从而增加解的多样性，尽可能跳出局部最优。

---

具体而言，引入余[弦相似度](https://www.sciencedirect.com/topics/computer-science/cosine-similarity)因子，通过比较当前生成的谐波向量与谐波存储器中最优向量之间的相似性来确定超参数调整的方向，如果相似度较高，则适当降低搜索概率，反之亦然，如具体改进部分的算法 1 所示。此外，针对多学习者堆叠问题，提出了性能指标d的综合确定。以所有基础学习者的平均表现为优化方向，并加入特征选择率等指标，以改善和平衡每个基础学习者的分类效果，寻求获得维度较低且适合大多数基础学习者的特征子集。该算法的具体步骤如下：

---

1. 初始化参数：和谐内存空间大小（HMS）、和谐向量维数（N），即原始特征数、和谐内存考虑率（HMCR）、音高调整率（PAR）、最大迭代次数（T）和基本学习器数（M）。

2. 初始化谐波存储器空间，生成HMS随机谐波向量。
   $$
   HM = \left|\begin{matrix}
       X^1 \\
       X^2 \\
       \vdots \\
       X^{HMS}
      \end{matrix} \right|
      = \left|\begin{matrix}
       x_1^1 & x_2^1 & \ldots & x_N^1 & |d(X^1) \\
       x_1^2 & x_2^2 & \ldots & x_N^2 & |d(X^2) \\
       \vdots \\
       x_1^{HMS} & x_2^{HMS} & \ldots & x_N^{HMS} & |d(X^{HMS})
      \end{matrix} \right|
   $$

---

3. 生成一个新的和谐向量。当[随机数](https://www.sciencedirect.com/topics/computer-science/random-number) r1 小于或等于 HMCR 时，则在谐波内存空间中搜索最优解向量的当前分量，并根据方程调整 HMCR。 [（3）](https://www.sciencedirect.com/science/article/pii/S1319157823004275#fd3).继续判断随机数r2是否小于或等于PAR;如果是，则反转当前分量变化并使用方程 [（4）](https://www.sciencedirect.com/science/article/pii/S1319157823004275#fd4) 调整 PAR。如果 r1 大于 HMCR，则随机生成当前分量。
4. 更新和谐内存空间。如果迭代生成的和谐向量的适应度函数值低于和谐记忆空间中最差向量，则该向量将被替换。
5. 连续重复 3-4 次，直到最大迭代次数𝑇达到并输出最佳特征子集。

---

### HS堆叠学习预测

在第三阶段，该文提出了一种基于HS的堆叠算法，该算法利用HS优化堆叠上层模型中基本学习者组合的选择。

---

具体来说，我们提出了一种方法来自动选择更适合当前降维数据集的基本学习器子集。如上所述，在特征选择阶段，我们选择了在所有基础学习器中相对平衡的特征子集，但仍有一些学习器性能较差，过多的基础学习器会增加算法的开销。分类器选择问题与特征选择问题类似，都是面向分类性能的[组合优化问题](https://www.sciencedirect.com/topics/computer-science/combinatorial-optimization-problem)。因此，我们也引入了HS来优化堆叠，但与AHHS不同的是，该算法不再使用余弦相似度因子来确定超参数调整方向，而是使用当前位置指数所表示的基学习器的精度作为变化因子。具体的改进如算法 2 所示。此阶段的具体流程如下：

---

1. 初始化参数：和谐记忆空间大小（HMS）、和谐向量维数（M），即基本学习器数量、和谐记忆考虑率（HMCR）、音高调整率（PAR）和最大迭代次数（T）。
2. 初始化 Harmony 内存空间，生成 HMS 随机 Harmony 向量。适应度函数设计采用双目标形式，分别以堆叠分类准确率和选取基础学习者数量作为[评价指标](https://www.sciencedirect.com/topics/computer-science/evaluation-metric)。哪里A𝑐𝑐表示元预测变量的分类精度，M 是基础学习者的数量，𝑥我是我第位置和声向量分量值（0 或 1）。在本文中，𝜆设置为 0.9。

---

3. 生成一个新的和谐向量。当随机数r1小于或等于HMCR时，搜索和谐内存空间中最优解向量的当前分量取值，并继续判断随机数r2是否小于或等于PAR。如果是，则当前组件变化被反转;如果 r1 大于 HMCR，则随机生成当前分量。哪里`A𝑐𝑐 i`是`i th`基础学习器。这意味着当前基础学习器的准确率越高，其选择的概率就越高。和𝛽是调整因子。
4. 更新和谐内存空间。如果迭代生成的和谐向量的适应度函数值低于和谐记忆空间中最差向量，则该向量将被替换。
5. 连续重复 3-4 次，直到最大迭代次数𝑇达到并输出最佳基础学习器子集。

---

### 提高模型的整体性能

将所有基础学习者的**平均性能作为特征**选择目标，并使用**自适应超参数策略**来加速迭代过程。然后使用 **HS** 进行优化，以找到基本学习器的最佳组合，从而提高模型性能，同时降低复杂性。

---

## 实验结果与分析

本研究利用PIDD数据集进行了一系列全面的实验和后续结果分析。这样做是为了对拟议的方法和[基准模型](https://www.sciencedirect.com/topics/computer-science/model-benchmark)进行直接比较评估。此外，还设计了**消融实验**，以证实所提出策略的有效性和框架内每个阶段的不可或缺性。此外，该方法的应用被扩展到一个独家数据集，从而为AHDHS-Stacking的性能提供了额外的验证。这种扩展也促进了对影响糖尿病诊断的关键因素的探索。

---

## 对比

### 实验设置

本节的实验将本文提出的AHDHS-Stacking框架与一般的个体预测变量和常见的集成模型（如随机森林、Bagging、Adaboost等）进行了比较。实验采用5重交叉验证方式进行，为保证实验的公平性，本文将HS应用于所有比较预测模型进行特征选择。需要注意的是，上述使用HS的相关实验均独立运行20次，均值作为最终[分类结果](https://www.sciencedirect.com/topics/computer-science/classification-result)。HS的实验参数如[表3](https://www.sciencedirect.com/science/article/pii/S1319157823004275#tbl3)所示。

---

此外，本研究还与一些最先进的方法进行了比较，包括 NSGA-II-Stacking（[Singh 和 Singh，2020](https://www.sciencedirect.com/science/article/pii/S1319157823004275#b79) 年）、PMSGD（[Azad 等人，2022](https://www.sciencedirect.com/science/article/pii/S1319157823004275#b3) 年）、GA-Stacking3（[Ledezma 等人，2004](https://www.sciencedirect.com/science/article/pii/S1319157823004275#b46) 年）、HM-BagMoov（[Bashir 等人，2016](https://www.sciencedirect.com/science/article/pii/S1319157823004275#b4) 年）等。上述所有最先进的算法都使用原始文章中的设置和最高性能。所有实验均在配备 Intel（R） Core（TM） I7-10700@2.90 GHz 处理器和 32 GB RAM 的计算机上使用 Python 3.8 进行。

---

### 评估标准

在进行的实验中，采用了五个指标来促进所提出的AHDHS-Stacking模型与替代方法之间的全面比较。这五个指标是：准确率、召回率、精确度、F 测量和 MCC。

---

其中TP是[真阳性](https://www.sciencedirect.com/topics/computer-science/true-positive)实例数，TN是真阴性实例数，FN是[假阴性](https://www.sciencedirect.com/topics/computer-science/false-negative)实例数，FP是假[阳性实例](https://www.sciencedirect.com/topics/computer-science/positive-instance)数。准确度表示所有正确预测（正和负）在所有样本中的比例。精度表示所有正确预测与所有正预测的比例。召回率表示所有正确预测与所有实际正预测的比例。F-measure 是精确度和召回率的组合，用于评估预测变量。MCC 是描述实际分类和预测分类的相关系数。该值越接近 1，预测越准确，可用于类别不平衡的情况，更适合 PIDD 和 CWMDD。
$$
Accuracy = \frac{TP + TN}{TP + FP + FN + TN}
$$

$$
Precision = \frac{TP}{TP + FP}
$$

$$
Recall = \frac{TP}{TP + FN}
$$

$$
F - measure = \frac{TP}{TP + \frac{FN + FP}{2}}
$$

$$
MCC = \frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP) \times (TP + FN) \times (TN + FP) \times (TN + FN}}
$$